{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "nqueries = 100 # number of unique queries\n",
    "p_phi_right = 0.3 # proportion of queries where the algorithm's phi checks out\n",
    "algo_accuracy = 0.7 # algorithm marginal accuracy\n",
    "human_accuracy = 0.6 # human marginal accuracy\n",
    "\n",
    "def experiment(nqueries, algo_accuracy, human_accuracy, p_phi_right):\n",
    "    # Based on algorithm marginal accuracy, randomly assign tasks to be correct and incorrect for algorithm\n",
    "    algo_correct = np.zeros(nqueries)\n",
    "    algo_correct[:int(algo_accuracy * nqueries)] = 1\n",
    "    algo_correct = np.random.permutation(algo_correct)\n",
    "\n",
    "    # Based on p_phi_right, randomly assign tasks to be ones where phi checks out\n",
    "    phi_right = np.zeros(nqueries)\n",
    "    phi_right[:int(p_phi_right * nqueries)] = 1\n",
    "    phi_right = np.random.permutation(phi_right)\n",
    "\n",
    "    # Based on human marginal accuracy, randomly assign tasks to be correct and incorrect for human\n",
    "    human_correct = np.zeros(nqueries)\n",
    "    human_correct[:int(human_accuracy * nqueries)] = 1\n",
    "    human_correct = np.random.permutation(human_correct)\n",
    "\n",
    "    # Calculate probability that algorithm is correct given phi checks out\n",
    "    p_correct_given_phi = np.mean(algo_correct[np.where(phi_right)])\n",
    "\n",
    "    # Performance given human knows p_correct_given_phi (but human doesn't know algo_accuracy!)\n",
    "    # The optimal strategy is:\n",
    "    # if human_accuracy > p_correct_given_phi, then the human should just trust themselves\n",
    "    # else, the human should trust the algorithm when phi checks out and trust themselves otherwise\n",
    "\n",
    "    if p_correct_given_phi <= human_accuracy:\n",
    "        performance = human_accuracy\n",
    "        return False, False\n",
    "    else:\n",
    "        combined = human_correct.copy()\n",
    "        phi_right_i = np.where(phi_right) # get all the indices where phi checks out\n",
    "        combined[phi_right_i] = algo_correct[phi_right_i] # for all the indices where phi checks out, take algo's decision\n",
    "        performance = np.mean(combined)\n",
    "\n",
    "        better_than_human = performance > human_accuracy\n",
    "        better_than_algo = performance > algo_accuracy\n",
    "        return better_than_human, better_than_algo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments():\n",
    "    phi_values = [0.3, 0.7, 0.9]\n",
    "    algo_accuracy_values = [0.3, 0.7, 0.9]\n",
    "    human_accuracy_values = [0.3, 0.7, 0.9]\n",
    "\n",
    "    nqueries = 100\n",
    "    niters = 10000\n",
    "\n",
    "    for phi in phi_values:\n",
    "        for algo_accuracy in algo_accuracy_values:\n",
    "            for human_accuracy in human_accuracy_values:\n",
    "                exceeds_human = np.zeros(niters)\n",
    "                exceeds_algo = np.zeros(niters)\n",
    "                complementary = np.zeros(niters)\n",
    "                for i in range(niters):\n",
    "                    better_than_human, better_than_algo = experiment(nqueries, algo_accuracy, human_accuracy, phi)\n",
    "                    exceeds_human[i] = better_than_human\n",
    "                    exceeds_algo[i] = better_than_algo\n",
    "                    complementary[i] = better_than_human and better_than_algo\n",
    "\n",
    "                print(f\"Phi: {phi}, Algo accuracy: {algo_accuracy}, Human accuracy: {human_accuracy}\")\n",
    "                print(f\"- Better than human: {np.mean(exceeds_human)}\")\n",
    "                print(f\"- Better than algo: {np.mean(exceeds_algo)}\")\n",
    "                print(f\"- Complementary: {np.mean(complementary)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi: 0.3, Algo accuracy: 0.3, Human accuracy: 0.3\n",
      "- Better than human: 0.3018\n",
      "- Better than algo: 0.3018\n",
      "- Complementary: 0.3018\n",
      "Phi: 0.3, Algo accuracy: 0.3, Human accuracy: 0.7\n",
      "- Better than human: 0.0\n",
      "- Better than algo: 0.0\n",
      "- Complementary: 0.0\n",
      "Phi: 0.3, Algo accuracy: 0.3, Human accuracy: 0.9\n",
      "- Better than human: 0.0\n",
      "- Better than algo: 0.0\n",
      "- Complementary: 0.0\n",
      "Phi: 0.3, Algo accuracy: 0.7, Human accuracy: 0.3\n",
      "- Better than human: 1.0\n",
      "- Better than algo: 0.0\n",
      "- Complementary: 0.0\n",
      "Phi: 0.3, Algo accuracy: 0.7, Human accuracy: 0.7\n",
      "- Better than human: 0.2948\n",
      "- Better than algo: 0.2948\n",
      "- Complementary: 0.2948\n",
      "Phi: 0.3, Algo accuracy: 0.7, Human accuracy: 0.9\n",
      "- Better than human: 0.0\n",
      "- Better than algo: 0.0003\n",
      "- Complementary: 0.0\n",
      "Phi: 0.3, Algo accuracy: 0.9, Human accuracy: 0.3\n",
      "- Better than human: 1.0\n",
      "- Better than algo: 0.0\n",
      "- Complementary: 0.0\n",
      "Phi: 0.3, Algo accuracy: 0.9, Human accuracy: 0.7\n",
      "- Better than human: 0.9847\n",
      "- Better than algo: 0.0\n",
      "- Complementary: 0.0\n",
      "Phi: 0.3, Algo accuracy: 0.9, Human accuracy: 0.9\n",
      "- Better than human: 0.2679\n",
      "- Better than algo: 0.2679\n",
      "- Complementary: 0.2679\n",
      "Phi: 0.7, Algo accuracy: 0.3, Human accuracy: 0.3\n",
      "- Better than human: 0.2929\n",
      "- Better than algo: 0.2929\n",
      "- Complementary: 0.2929\n",
      "Phi: 0.7, Algo accuracy: 0.3, Human accuracy: 0.7\n",
      "- Better than human: 0.0\n",
      "- Better than algo: 0.0\n",
      "- Complementary: 0.0\n",
      "Phi: 0.7, Algo accuracy: 0.3, Human accuracy: 0.9\n",
      "- Better than human: 0.0\n",
      "- Better than algo: 0.0\n",
      "- Complementary: 0.0\n",
      "Phi: 0.7, Algo accuracy: 0.7, Human accuracy: 0.3\n",
      "- Better than human: 1.0\n",
      "- Better than algo: 0.0\n",
      "- Complementary: 0.0\n",
      "Phi: 0.7, Algo accuracy: 0.7, Human accuracy: 0.7\n",
      "- Better than human: 0.3041\n",
      "- Better than algo: 0.3041\n",
      "- Complementary: 0.3041\n",
      "Phi: 0.7, Algo accuracy: 0.7, Human accuracy: 0.9\n",
      "- Better than human: 0.0\n",
      "- Better than algo: 0.0\n",
      "- Complementary: 0.0\n",
      "Phi: 0.7, Algo accuracy: 0.9, Human accuracy: 0.3\n",
      "- Better than human: 1.0\n",
      "- Better than algo: 0.0\n",
      "- Complementary: 0.0\n",
      "Phi: 0.7, Algo accuracy: 0.9, Human accuracy: 0.7\n",
      "- Better than human: 1.0\n",
      "- Better than algo: 0.0034\n",
      "- Complementary: 0.0034\n",
      "Phi: 0.7, Algo accuracy: 0.9, Human accuracy: 0.9\n",
      "- Better than human: 0.2644\n",
      "- Better than algo: 0.2644\n",
      "- Complementary: 0.2644\n",
      "Phi: 0.9, Algo accuracy: 0.3, Human accuracy: 0.3\n",
      "- Better than human: 0.2693\n",
      "- Better than algo: 0.2693\n",
      "- Complementary: 0.2693\n",
      "Phi: 0.9, Algo accuracy: 0.3, Human accuracy: 0.7\n",
      "- Better than human: 0.0\n",
      "- Better than algo: 0.0\n",
      "- Complementary: 0.0\n",
      "Phi: 0.9, Algo accuracy: 0.3, Human accuracy: 0.9\n",
      "- Better than human: 0.0\n",
      "- Better than algo: 0.0\n",
      "- Complementary: 0.0\n",
      "Phi: 0.9, Algo accuracy: 0.7, Human accuracy: 0.3\n",
      "- Better than human: 1.0\n",
      "- Better than algo: 0.0123\n",
      "- Complementary: 0.0123\n",
      "Phi: 0.9, Algo accuracy: 0.7, Human accuracy: 0.7\n",
      "- Better than human: 0.2628\n",
      "- Better than algo: 0.2628\n",
      "- Complementary: 0.2628\n",
      "Phi: 0.9, Algo accuracy: 0.7, Human accuracy: 0.9\n",
      "- Better than human: 0.0\n",
      "- Better than algo: 0.0\n",
      "- Complementary: 0.0\n",
      "Phi: 0.9, Algo accuracy: 0.9, Human accuracy: 0.3\n",
      "- Better than human: 1.0\n",
      "- Better than algo: 0.0001\n",
      "- Complementary: 0.0001\n",
      "Phi: 0.9, Algo accuracy: 0.9, Human accuracy: 0.7\n",
      "- Better than human: 1.0\n",
      "- Better than algo: 0.0623\n",
      "- Complementary: 0.0623\n",
      "Phi: 0.9, Algo accuracy: 0.9, Human accuracy: 0.9\n",
      "- Better than human: 0.2075\n",
      "- Better than algo: 0.2075\n",
      "- Complementary: 0.2075\n"
     ]
    }
   ],
   "source": [
    "print(\"Each block gives the results of 10,000 iterations \")\n",
    "run_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
